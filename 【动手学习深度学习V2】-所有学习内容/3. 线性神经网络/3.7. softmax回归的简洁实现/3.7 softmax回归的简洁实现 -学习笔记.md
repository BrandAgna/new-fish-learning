## Softmax回归的简洁实现

### 3.7.1 初始化模型参数
- Softmax回归的输出层是一个全连接层。只需在`Sequential`中添加一个带有10个输出的全连接层，并随机初始化权重。

### 3.7.2 重新审视Softmax的实现
- 指出从计算角度来看，指数可能会造成数值稳定性问题。解决方案是在计算softmax前从所有`o_k`中减去`max(o_k)`。

### 3.7.3 优化算法
- 使用学习率为0.1的小批量随机梯度下降作为优化算法，这显示了优化器的普适性。

### 3.7.4 训练
- 调用3.6节中定义的训练函数来训练模型，进行了10个迭代周期的训练。

### 3.7.5 小结
- 使用深度学习框架的高级API可以更简洁地实现softmax回归。框架采取了额外的措施以确保数值稳定性，避免了从零开始实现时可能遇到的问题。
