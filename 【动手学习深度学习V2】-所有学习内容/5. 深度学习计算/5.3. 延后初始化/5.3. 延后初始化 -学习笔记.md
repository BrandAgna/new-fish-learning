### 学习笔记 - 深度学习计算: 延迟初始化

#### 概述
本章节介绍了深度学习中的延迟初始化机制，即在网络定义时不立即初始化参数，而是在得到输入数据的尺寸后再进行初始化。这种机制在处理复杂模型时尤为重要。

#### 关键概念
- **延迟初始化**:
  - 一种在网络定义时不指定输入和输出尺寸，而在接收到实际数据后再初始化参数的机制。
  - 减轻了手动指定每层输入输出尺寸的工作量。
- **初始化触发**:
  - 当网络第一次接收到输入数据时，根据输入数据的尺寸自动推断每层的参数尺寸并进行初始化。
  - 这种机制确保了参数尺寸的准确性和灵活性。

#### 实际问题与挑战
- 描述了如何在不同的网络架构中应用延迟初始化。
- 探讨了在特殊情况下，如共享参数或自定义层时，如何正确处理延迟初始化。

#### 方法和策略
- **自定义层与延迟初始化**:
  - 展示了在自定义层中如何处理延迟初始化。
  - 讨论了在这些情况下保持参数初始化的一致性的策略。
- **共享参数的处理**:
  - 讨论了在使用延迟初始化时如何处理参数共享的情况。
  - 描述了共享参数和延迟初始化之间的相互作用。

#### 总结
延迟初始化是深度学习框架中的一个重要特性，它大大简化了复杂模型的定义和实验过程。理解这一机制对于构建灵活且高效的神经网络模型非常重要。
