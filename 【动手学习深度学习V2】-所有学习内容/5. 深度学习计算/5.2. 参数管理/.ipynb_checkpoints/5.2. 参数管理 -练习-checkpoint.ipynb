{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ce83b1",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc8a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight torch.Size([20, 20])\n",
      "linear.bias torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "## 1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FancyMLP, self).__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = torch.mm(X, self.rand_weight) + 1\n",
    "        X = self.linear(X)\n",
    "        return X\n",
    "\n",
    "fancy_net = FancyMLP()\n",
    "for name, param in fancy_net.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97101a0d",
   "metadata": {},
   "source": [
    "## 2. 查看初始化模块文档以了解不同的初始化方法。\n",
    "当查看神经网络参数初始化的相关文档时，会发现存在多种方法来初始化模型的权重和偏置。下面将叙述几种常见的初始化方法：\n",
    "\n",
    "1. **零初始化（Zero Initialization）**:\n",
    "   - 使用零来初始化权重和偏置。\n",
    "   - `nn.init.zeros_(tensor)`：将给定的 tensor 初始化为全零。\n",
    "\n",
    "2. **均匀分布初始化（Uniform Initialization）**:\n",
    "   - 权重和偏置初始化为在给定范围内均匀分布的随机数。\n",
    "   - `nn.init.uniform_(tensor, a=0.0, b=1.0)`：使用从 `[a, b)` 范围内的均匀分布生成的值来填充 tensor。\n",
    "\n",
    "3. **正态分布初始化（Normal Initialization）**:\n",
    "   - 权重和偏置初始化为服从正态分布的随机数。\n",
    "   - `nn.init.normal_(tensor, mean=0.0, std=1.0)`：使用指定均值和标准差的正态分布填充 tensor。\n",
    "\n",
    "4. **Xavier/Glorot 初始化**:\n",
    "   - 用于保持输入和输出方差一致，适用于传统的激活函数（如sigmoid，tanh）。\n",
    "   - `nn.init.xavier_uniform_(tensor)`：使用均匀分布。\n",
    "   - `nn.init.xavier_normal_(tensor)`：使用正态分布。\n",
    "\n",
    "5. **He/Kaiming 初始化**:\n",
    "   - 特别适用于ReLU激活函数，有助于解决深层网络中的梯度消失或爆炸问题。\n",
    "   - `nn.init.kaiming_uniform_(tensor, mode='fan_in', nonlinearity='relu')`：使用均匀分布。\n",
    "   - `nn.init.kaiming_normal_(tensor, mode='fan_in', nonlinearity='relu')`：使用正态分布。\n",
    "\n",
    "6. **常数初始化（Constant Initialization）**:\n",
    "   - 使用常数来初始化权重和偏置。\n",
    "   - `nn.init.constant_(tensor, val)`：使用给定的值 `val` 填充 tensor。\n",
    "\n",
    "每种初始化方法都有其适用的场景和特定的目的，选择哪种方法通常取决于网络的架构和所使用的激活函数。正确的初始化策略可以显著提升模型的学习效率和性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55320327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.035850875079631805\n",
      "Epoch 1: Loss: 0.05273083597421646\n",
      "Epoch 2: Loss: 0.04683181270956993\n",
      "Epoch 3: Loss: 0.21679353713989258\n",
      "Epoch 4: Loss: 0.018026290461421013\n",
      "Epoch 5: Loss: 0.13909339904785156\n",
      "Epoch 6: Loss: 0.018563684076070786\n",
      "Epoch 7: Loss: 0.13122691214084625\n",
      "Epoch 8: Loss: 0.05326090008020401\n",
      "Epoch 9: Loss: 0.00917595624923706\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "## 3. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "# 定义共享层\n",
    "shared = nn.Linear(8, 8)\n",
    "\n",
    "# 构建模型\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "\n",
    "# 模拟输入数据\n",
    "X = torch.rand(size=(2, 4))\n",
    "\n",
    "# 模拟训练过程\n",
    "def train(net, X):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        output = net(X)\n",
    "        loss = criterion(output, torch.rand(2, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch}: Loss: {loss.item()}')\n",
    "\n",
    "train(net, X)\n",
    "\n",
    "# 检查共享层的参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140b048",
   "metadata": {},
   "source": [
    "## 练习 4: 为什么共享参数是个好主意？\n",
    "共享参数可以减少模型的参数数量，从而降低过拟合的风险和计算成本。此外，在处理具有重复模式的数据时（如文本或图像），共享参数可以帮助模型更好地学习这些模式。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
