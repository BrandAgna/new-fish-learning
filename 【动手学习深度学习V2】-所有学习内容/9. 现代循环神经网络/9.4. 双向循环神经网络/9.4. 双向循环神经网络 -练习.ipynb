{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091c6279",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 如果不同方向使用不同数量的隐藏单位，$\\mathbf{H_t}$的形状会发生怎样的变化？\n",
    "1. 设计一个具有多个隐藏层的双向循环神经网络。\n",
    "1. 在自然语言中一词多义很常见。例如，“bank”一词在不同的上下文“i went to the bank to deposit cash”和“i went to the bank to sit down”中有不同的含义。如何设计一个神经网络模型，使其在给定上下文序列和单词的情况下，返回该单词在此上下文中的向量表示？哪种类型的神经网络架构更适合处理一词多义？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7738fc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "# 定义一个简化的双向LSTM模型，使用不同数量的隐藏单元\n",
    "class SimpleBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens_forward, num_hiddens_backward, num_layers):\n",
    "        super(SimpleBiLSTM, self).__init__()\n",
    "        # 分别定义前向和后向的LSTM层\n",
    "        self.lstm_forward = nn.LSTM(vocab_size, num_hiddens_forward, num_layers, batch_first=True)\n",
    "        self.lstm_backward = nn.LSTM(vocab_size, num_hiddens_backward, num_layers, batch_first=True)\n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(num_hiddens_forward + num_hiddens_backward, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 分别处理前向和后向的数据\n",
    "        out_forward, _ = self.lstm_forward(x)\n",
    "        out_backward, _ = self.lstm_backward(torch.flip(x, [1]))\n",
    "        # 合并前向和后向的输出\n",
    "        out = torch.cat((out_forward, out_backward), -1)\n",
    "        # 通过全连接层\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 设置模型参数\n",
    "vocab_size = 1024  # 假设的词汇量大小\n",
    "num_hiddens_forward = 128\n",
    "num_hiddens_backward = 256\n",
    "num_layers = 2\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleBiLSTM(vocab_size, num_hiddens_forward, num_hiddens_backward, num_layers)\n",
    "\n",
    "# 创建一个随机输入张量来测试模型\n",
    "test_input = torch.rand(32, 35, vocab_size)  # 批量大小为32，时间步为35\n",
    "test_output = model(test_input)\n",
    "\n",
    "test_output.shape  # 检查输出张量的形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1f251e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 练习2\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiLayerBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_layers):\n",
    "        super(MultiLayerBiLSTM, self).__init__()\n",
    "        # 定义多层双向LSTM\n",
    "        self.lstm = nn.LSTM(vocab_size, num_hiddens, num_layers, bidirectional=True)\n",
    "        # 定义输出层\n",
    "        self.fc = nn.Linear(num_hiddens * 2, vocab_size)  # 乘以2是因为双向\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM层\n",
    "        output, _ = self.lstm(x)\n",
    "        # 通过全连接层\n",
    "        out = self.fc(output)\n",
    "        return out\n",
    "\n",
    "# 参数设置\n",
    "vocab_size = 1024  # 假设的词汇量大小\n",
    "num_hiddens = 256\n",
    "num_layers = 3  # 多层\n",
    "\n",
    "# 创建模型实例\n",
    "model = MultiLayerBiLSTM(vocab_size, num_hiddens, num_layers)\n",
    "\n",
    "# 测试模型\n",
    "test_input = torch.rand(32, 35, vocab_size)  # 批量大小为32，时间步为35\n",
    "test_output = model(test_input)\n",
    "\n",
    "test_output.shape  # 输出形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac66aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6838fca41d24fb7aa0de0eaff1d555e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14591\\anaconda3\\envs\\python3.9\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\14591\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840db15faf7f45cfb5dabe7d3b462085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8350f969c34c418592d608ffa6beaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9040c98e2aaf44ccb5e28bba03bbe78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cb744ccc6845979ea9246d37f0e546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 练习3\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 初始化BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 示例句子\n",
    "sentence = \"I went to the bank to deposit cash.\"\n",
    "\n",
    "# 对句子进行分词，并获取各个词的嵌入\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# outputs中包含了句子中每个词的上下文相关嵌入\n",
    "word_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704fdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
