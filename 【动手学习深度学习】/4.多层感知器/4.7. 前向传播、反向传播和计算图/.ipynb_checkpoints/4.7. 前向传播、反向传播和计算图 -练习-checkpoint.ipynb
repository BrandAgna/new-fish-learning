{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed6d9cb",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 练习\n",
    "\n",
    "1. 假设一些标量函数$\\mathbf{X}$的输入$\\mathbf{X}$是$n \\times m$矩阵。$f$相对于$\\mathbf{X}$的梯度维数是多少？\n",
    "1. 向本节中描述的模型的隐藏层添加偏置项（不需要在正则化项中包含偏置项）。\n",
    "    1. 画出相应的计算图。\n",
    "    1. 推导正向和反向传播方程。\n",
    "1. 计算本节所描述的模型，用于训练和预测的内存占用。\n",
    "1. 假设想计算二阶导数。计算图发生了什么？预计计算需要多长时间？\n",
    "1. 假设计算图对当前拥有的GPU来说太大了。\n",
    "    1. 请试着把它划分到多个GPU上。\n",
    "    1. 与小批量训练相比，有哪些优点和缺点？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9b57b",
   "metadata": {},
   "source": [
    "### 练习 1\n",
    "\n",
    "假设函数 \\( f \\) 的输入 \\( \\mathbf{X} \\) 是一个 \\( n \\times m \\) 矩阵。\\( f \\) 相对于 \\( \\mathbf{X} \\) 的梯度的维度是 \\( n \\times m \\)。梯度的每个元素 \\( \\frac{\\partial f}{\\partial X_{ij}} \\) 对应于 \\( \\mathbf{X} \\) 中的元素 \\( X_{ij} \\)。因此，梯度矩阵与 \\( \\mathbf{X} \\) 具有相同的维度。\n",
    "\n",
    "### 练习 2\n",
    "\n",
    "向隐藏层添加偏置项并不会影响计算图的总体结构，但会在每个隐藏层的输出中添加一个额外的步骤。\n",
    "\n",
    "#### 计算图\n",
    "\n",
    "假设我们有一个单隐藏层的网络，其中 \\( \\mathbf{X} \\) 是输入，\\( \\mathbf{H} \\) 是隐藏层的输出，\\( \\mathbf{Y} \\) 是网络的输出。计算图如下：\n",
    "\n",
    "1. \\( \\mathbf{H} = \\text{activation}(\\mathbf{XW}_1 + \\mathbf{b}_1) \\)\n",
    "2. \\( \\mathbf{Y} = \\mathbf{HW}_2 + \\mathbf{b}_2 \\)\n",
    "\n",
    "其中 \\( \\mathbf{W}_1, \\mathbf{W}_2 \\) 是权重矩阵，\\( \\mathbf{b}_1, \\mathbf{b}_2 \\) 是偏置项。\n",
    "\n",
    "#### 正向和反向传播方程\n",
    "\n",
    "- **正向传播**：\n",
    "    - 隐藏层：\\( \\mathbf{H} = \\text{activation}(\\mathbf{XW}_1 + \\mathbf{b}_1) \\)\n",
    "    - 输出层：\\( \\mathbf{Y} = \\mathbf{HW}_2 + \\mathbf{b}_2 \\)\n",
    "- **反向传播**：\n",
    "    - 输出层梯度：\\( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}} \\)\n",
    "    - 隐藏层梯度：\\( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{H}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}} \\mathbf{W}_2^\\top \\)\n",
    "    - 权重 \\( \\mathbf{W}_2 \\) 的梯度：\\( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_2} = \\mathbf{H}^\\top \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}} \\)\n",
    "    - 偏置 \\( \\mathbf{b}_2 \\) 的梯度：\\( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_2} = \\text{sum}(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}, \\text{axis}=0) \\)\n",
    "\n",
    "对于隐藏层的权重和偏置的梯度，可以用类似的方式计算。\n",
    "\n",
    "### 练习 3\n",
    "\n",
    "计算内存占用需要考虑模型中的所有参数和中间变量。对于本节描述的模型，假设我们有两个权重矩阵 \\( \\mathbf{W}_1 \\) 和 \\( \\mathbf{W}_2 \\)，以及两个偏置向量 \\( \\mathbf{b}_1 \\) 和 \\( \\mathbf{b}_2 \\)。内存占用大致为：\n",
    "\n",
    "- 每个权重矩阵和偏置向量的存储。\n",
    "- 每层的输入和输出（例如，\\( \\mathbf{X}, \\mathbf{H}, \\mathbf{Y} \\)）。\n",
    "- 每层梯度的存储（在训练期间）。\n",
    "\n",
    "具体的内存计算将取决于这些矩阵和向量的维度以及使用的数据类型（例如，float32或float64）。\n",
    "\n",
    "### 练习 4\n",
    "\n",
    "\n",
    "\n",
    "计算二阶导数意味着我们需要对每个参数的梯度再次进行微分。这将导致计算图变得更加复杂，并且计算成本会显著增加。\n",
    "\n",
    "- **计算图**：对于每个原始梯度，都会有一个额外的节点表示其导数（即二阶导数）。\n",
    "- **计算时间**：预计计算时间将比仅计算一阶导数的情况大得多，因为对于每个参数，都需要进行额外的导数计算。\n",
    "\n",
    "### 练习 5\n",
    "\n",
    "如果计算图对当前拥有的GPU来说太大：\n",
    "\n",
    "#### 1. 将它划分到多个GPU上\n",
    "\n",
    "- **优点**：\n",
    "    - 允许训练更大的模型。\n",
    "    - 可能通过并行处理提高训练速度。\n",
    "- **缺点**：\n",
    "    - 增加了实施的复杂性，需要处理跨GPU的数据传输。\n",
    "    - 可能由于GPU之间的通信而导致效率降低。\n",
    "\n",
    "#### 2. 与小批量训练相比\n",
    "\n",
    "- **优点**：\n",
    "    - 能够处理更大的数据集和模型。\n",
    "    - 可以利用更多的计算资源。\n",
    "- **缺点**：\n",
    "    - 需要更复杂的并行计算策略。\n",
    "    - 通信开销可能会减少扩展效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d74dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
