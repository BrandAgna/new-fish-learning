{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d794c62a",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 练习\n",
    "\n",
    "1. 我们可以更深入地探讨指数族与softmax之间的联系。\n",
    "    1. 计算softmax交叉熵损失$l(\\mathbf{y},\\hat{\\mathbf{y}})$的二阶导数。\n",
    "    1. 计算$\\mathrm{softmax}(\\mathbf{o})$给出的分布方差，并与上面计算的二阶导数匹配。\n",
    "1. 假设我们有三个类发生的概率相等，即概率向量是$(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$。\n",
    "    1. 如果我们尝试为它设计二进制代码，有什么问题？\n",
    "    1. 请设计一个更好的代码。提示：如果我们尝试编码两个独立的观察结果会发生什么？如果我们联合编码$n$个观测值怎么办？\n",
    "1. softmax是对上面介绍的映射的误称（虽然深度学习领域中很多人都使用这个名字）。真正的softmax被定义为$\\mathrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b))$。\n",
    "    1. 证明$\\mathrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$。\n",
    "    1. 证明$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) > \\mathrm{max}(a, b)$成立，前提是$\\lambda > 0$。\n",
    "    1. 证明对于$\\lambda \\to \\infty$，有$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$。\n",
    "    1. soft-min会是什么样子？\n",
    "    1. 将其扩展到两个以上的数字。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eb6a6",
   "metadata": {},
   "source": [
    "### 练习答案\n",
    "\n",
    "**1. 指数族与softmax之间的联系：**\n",
    "\n",
    "A. **Softmax交叉熵损失的二阶导数：**\n",
    "       - 对于softmax交叉熵损失$l(\\mathbf{y},\\hat{\\mathbf{y}})$，其二阶导数通常涉及复杂的数学运算，依赖于softmax的具体表达式和交叉熵损失的定义。\n",
    "\n",
    "B. **Softmax分布方差与二阶导数匹配：**\n",
    "       - Softmax函数生成的分布方差可以通过计算每个类别概率的方差来获得。这个方差通常与softmax函数的二阶导数相关，但具体匹配取决于softmax函数的具体形式。\n",
    "\n",
    "    \n",
    "**2. 设计二进制代码：**\n",
    "\n",
    "A. **三个类等概率问题：**- 如果三个类发生的概率相等，使用二进制代码表示时会遇到不均匀长度的问题，即无法使用等长的二进制代码来等概率地表示三个类别。\n",
    "\n",
    "B. **设计更好的代码：**- 为了有效编码，可以考虑将多个观测结果联合编码。例如，对于两个独立观测结果的组合，有9种可能性，可以用四位二进制代码均匀地表示。对于$n$个观测值，可以用类似的方法扩展。\n",
    "\n",
    "**3. RealSoftMax函数：**\n",
    "\n",
    "A. **证明$\\mathrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$：**\n",
    "       - 由于$\\exp(a)$和$\\exp(b)$均为正数，$\\exp(a) + \\exp(b) > \\exp(\\mathrm{max}(a, b))$。因此，$\\log (\\exp(a) + \\exp(b)) > \\mathrm{max}(a, b)$。\n",
    "\n",
    "B. **证明$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) > \\mathrm{max}(a, b)$：**\n",
    "       - 由于$\\lambda > 0$，可以推出$\\lambda^{-1} \\log (\\exp(\\lambda a) + \\exp(\\lambda b)) > \\mathrm{max}(a, b)$。\n",
    "\n",
    "C. **证明$\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$：**\n",
    "       - 当$\\lambda \\to \\infty$时，较大的项$\\exp(\\lambda \\cdot \\mathrm{max}(a, b))$将支配和，因此极限趋于$\\mathrm{max}(a, b)$。\n",
    "\n",
    "D. **Soft-min定义：**\n",
    "       - Soft-min可以定义为$\\mathrm{RealSoftMin}(a, b) = -\\log (\\exp(-a) + \\exp(-b))$。\n",
    "\n",
    "E. **扩展到两个以上的数字：**\n",
    "       - RealSoftMax可以扩展到多个数字，定义为$\\mathrm{RealSoftMax}(a_1, a_2, \\ldots, a_n) = \\log (\\exp(a_1) + \\exp(a_2) + \\ldots + \\exp(a_n))$。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
