{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a45b5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class ReluActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        #return weighted_input\n",
    "        return max(0, weighted_input)\n",
    "\n",
    "    def backward(self, output):\n",
    "        return 1 if output > 0 else 0\n",
    "\n",
    "\n",
    "class IdentityActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        return weighted_input\n",
    "\n",
    "    def backward(self, output):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class SigmoidActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        return 1.0 / (1.0 + np.exp(-weighted_input))\n",
    "\n",
    "    def backward(self, output):\n",
    "        return output * (1 - output)\n",
    "\n",
    "\n",
    "class TanhActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        return 2.0 / (1.0 + np.exp(-2 * weighted_input)) - 1.0\n",
    "\n",
    "    def backward(self, output):\n",
    "        return 1 - output * output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebde99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(object):\n",
    "    def __init__(self, input_width, input_height,\n",
    "                 channel_number, filter_width,\n",
    "                 filter_height, filter_number,\n",
    "                 zero_padding, stride, activator,\n",
    "                 learning_rae):\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        self.channel_number = channel_number\n",
    "        self.filter_width = filter_width\n",
    "        self.filter_height = filter_height\n",
    "        self.filter_number = filter_number\n",
    "        self.zero_padding = zero_padding\n",
    "        self.stride = stride\n",
    "        self.output_width = \\\n",
    "            ConvLayer.calculate_output_size(\n",
    "                self.input_width, filter_width, zero_padding,\n",
    "                stride)\n",
    "        self.output_height = \\\n",
    "            ConvLayer.calculate_output_size(self.input_height,filter_height,zero_padding,stride)\n",
    "        self.output_array = np.zeros((self.filter_number,self.output_height,self.output_width))\n",
    "        self.filters = []\n",
    "        for i in range(filter_number):\n",
    "            self.filters.append(Filter(filter_width,filter_height,self.channel_number))\n",
    "        self.activator = activator\n",
    "        self.learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2967f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def calculate_output_size(input_size, filter_size, zero_padding, stride):\n",
    "    return (input_size - filter_size + 2 * zero_padding) / stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1efb9a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d5c5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        # return weighted_input\n",
    "        return max(0, weighted_input)\n",
    "\n",
    "    def backward(self, output):\n",
    "        return 1 if output > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48ee8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input_array):\n",
    "    '''\n",
    "    计算卷积层的输出\n",
    "    输出结果保存在self.output_array\n",
    "    '''\n",
    "    self.input_array = input_array\n",
    "    self.padded_input_array = padding(input_array, self.zero_padding)\n",
    "    for f in range(self.filter_number):\n",
    "        filter = self.filters[f]\n",
    "        conv(self.padded_input_array, filter.get_weights(), self.output_array[f], self.stride, filter.get_bias())\n",
    "    element_wise_op(self.output_array, self.activator.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e942f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对numpy数组进行element wise操作\n",
    "def element_wise_op(array, op):\n",
    "    for i in np.nditer(array, op_flags=['readwrite']):\n",
    "        i[...] = op(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "994adfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(input_array, kernel_array, output_array, stride, bias):\n",
    "    '''\n",
    "    计算卷积，自动适配输入为2D和3D的情况\n",
    "    '''\n",
    "    channel_number = input_array.ndim\n",
    "    output_width = output_array.shape[1]\n",
    "    output_height = output_array.shape[0]\n",
    "    kernel_width = kernel_array.shape[-1]\n",
    "    kernel_height = kernel_array.shape[-2]\n",
    "\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            output_array[i][j] = (\n",
    "                get_patch(input_array, i, j, kernel_width, kernel_height, stride) * kernel_array\n",
    "            ).sum() + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "407187ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为数组增加Zero padding\n",
    "def padding(input_array, zp):\n",
    "    '''\n",
    "    为数组增加Zero padding，自动适配输入为2D和3D的情况\n",
    "    '''\n",
    "    if zp == 0:\n",
    "        return input_array\n",
    "    else:\n",
    "        if input_array.ndim == 3:\n",
    "            input_width = input_array.shape[2]\n",
    "            input_height = input_array.shape[1]\n",
    "            input_depth = input_array.shape[0]\n",
    "            padded_array = np.zeros((input_depth, input_height + 2 * zp, input_width + 2 * zp))\n",
    "            padded_array[:, zp : zp + input_height, zp : zp + input_width] = input_array\n",
    "            return padded_array\n",
    "        elif input_array.ndim == 2:\n",
    "            input_width = input_array.shape[1]\n",
    "            input_height = input_array.shape[0]\n",
    "            padded_array = np.zeros((input_height + 2 * zp, input_width + 2 * zp))\n",
    "            padded_array[zp : zp + input_height, zp : zp + input_width] = input_array\n",
    "            return padded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "802d5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_sensitivity_map(self, sensitivity_array, activator):\n",
    "    '''\n",
    "    计算传递到上一层的sensitivity map\n",
    "    sensitivity_array: 本层的sensitivity map\n",
    "    activator: 上一层的激活函数\n",
    "    '''\n",
    "    # 处理卷积步长，对原始sensitivity map进行扩展\n",
    "    expanded_array = self.expand_sensitivity_map(sensitivity_array)\n",
    "    # full卷积，对sensitivitiy map进行zero padding\n",
    "    # 虽然原始输入的zero padding单元也会获得残差\n",
    "    # 但这个残差不需要继续向上传递，因此就不计算了\n",
    "    expanded_width = expanded_array.shape[2]\n",
    "    zp = (self.input_width + self.filter_width - 1 - expanded_width) / 2\n",
    "    padded_array = padding(expanded_array, zp)\n",
    "    # 初始化delta_array，用于保存传递到上一层的\n",
    "    # sensitivity map\n",
    "    self.delta_array = self.create_delta_array()\n",
    "    # 对于具有多个filter的卷积层来说，最终传递到上一层的\n",
    "    # sensitivity map相当于所有的filter的\n",
    "    # sensitivity map之和\n",
    "    for f in range(self.filter_number):\n",
    "        filter = self.filters[f]\n",
    "        # 将filter权重翻转180度\n",
    "        flipped_weights = np.array(map(lambda i: np.rot90(i, 2), filter.get_weights()))\n",
    "        # 计算与一个filter对应的delta_array\n",
    "        delta_array = self.create_delta_array()\n",
    "        for d in range(delta_array.shape[0]):\n",
    "            conv(padded_array[f], flipped_weights[d], delta_array[d], 1, 0)\n",
    "        self.delta_array += delta_array\n",
    "    # 将计算结果与激活函数的偏导数做element-wise乘法操作\n",
    "    derivative_array = np.array(self.input_array)\n",
    "    element_wise_op(derivative_array, activator.backward)\n",
    "    self.delta_array *= derivative_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9a9cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_sensitivity_map(self, sensitivity_array):\n",
    "    depth = sensitivity_array.shape[0]\n",
    "    # 确定扩展后sensitivity map的大小\n",
    "    # 计算stride为1时sensitivity map的大小\n",
    "    expanded_width = (self.input_width - self.filter_width + 2 * self.zero_padding + 1)\n",
    "    expanded_height = (self.input_height - self.filter_height + 2 * self.zero_padding + 1)\n",
    "    # 构建新的sensitivity_map\n",
    "    expand_array = np.zeros((depth, expanded_height, expanded_width))\n",
    "    # 从原始sensitivity map拷贝误差值\n",
    "    for i in range(self.output_height):\n",
    "        for j in range(self.output_width):\n",
    "            i_pos = i * self.stride\n",
    "            j_pos = j * self.stride\n",
    "            expand_array[:, i_pos, j_pos] = sensitivity_array[:, i, j]\n",
    "    return expand_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58d3c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delta_array(self):\n",
    "    return np.zeros((self.channel_number, self.input_height, self.input_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f7e34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_gradient(self, sensitivity_array):\n",
    "    # 处理卷积步长，对原始sensitivity map进行扩展\n",
    "    expanded_array = self.expand_sensitivity_map(sensitivity_array)\n",
    "    for f in range(self.filter_number):\n",
    "        # 计算每个权重的梯度\n",
    "        filter = self.filters[f]\n",
    "        for d in range(filter.weights.shape[0]):\n",
    "            conv(self.padded_input_array[d], expanded_array[f], filter.weights_grad[d], 1, 0)\n",
    "        # 计算偏置项的梯度\n",
    "        filter.bias_grad = expanded_array[f].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82e7e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self):\n",
    "    '''\n",
    "    按照梯度下降，更新权重\n",
    "    '''\n",
    "    for filter in self.filters:\n",
    "        filter.update(self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12a87de6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ConvLayer' has no attribute 'calculate_output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 83\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights(\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): expected - actual \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m     80\u001b[0m                     d, i, j, expect_grad, cl\u001b[38;5;241m.\u001b[39mfilters[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweights_grad[d, i, j]))\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# 调用梯度检查函数\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[43mgradient_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[49], line 57\u001b[0m, in \u001b[0;36mgradient_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m error_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m o: o\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 计算forward值\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m a, b, cl \u001b[38;5;241m=\u001b[39m \u001b[43minit_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m cl\u001b[38;5;241m.\u001b[39mforward(a)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 求取sensitivity map，是一个全1数组\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 25\u001b[0m, in \u001b[0;36minit_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m      3\u001b[0m     [[[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m      4\u001b[0m     [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m     17\u001b[0m     [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]]])\n\u001b[0;32m     18\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m     19\u001b[0m     [[[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     20\u001b[0m     [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     24\u001b[0m     [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m]]])\n\u001b[1;32m---> 25\u001b[0m cl \u001b[38;5;241m=\u001b[39m \u001b[43mConvLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mIdentityActivator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m cl\u001b[38;5;241m.\u001b[39mfilters[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m     27\u001b[0m     [[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     28\u001b[0m     [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     35\u001b[0m     [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m     36\u001b[0m cl\u001b[38;5;241m.\u001b[39mfilters[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[36], line 16\u001b[0m, in \u001b[0;36mConvLayer.__init__\u001b[1;34m(self, input_width, input_height, channel_number, filter_width, filter_height, filter_number, zero_padding, stride, activator, learning_rae)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_padding \u001b[38;5;241m=\u001b[39m zero_padding\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m=\u001b[39m stride\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_width \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mConvLayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_output_size\u001b[49m(\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_width, filter_width, zero_padding,\n\u001b[0;32m     18\u001b[0m         stride)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_height \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m     20\u001b[0m     ConvLayer\u001b[38;5;241m.\u001b[39mcalculate_output_size(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_height,filter_height,zero_padding,stride)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_number,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_height,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_width))\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'ConvLayer' has no attribute 'calculate_output_size'"
     ]
    }
   ],
   "source": [
    "def init_test():\n",
    "    a = np.array(\n",
    "        [[[0,1,1,0,2],\n",
    "        [2,2,2,2,1],\n",
    "        [1,0,0,2,0],\n",
    "        [0,1,1,0,0],\n",
    "        [1,2,0,0,2]],\n",
    "        [[1,0,2,2,0],\n",
    "        [0,0,0,2,0],\n",
    "        [1,2,1,2,1],\n",
    "        [1,0,0,0,0],\n",
    "        [1,2,1,1,1]],\n",
    "        [[2,1,2,0,0],\n",
    "        [1,0,0,1,0],\n",
    "        [0,2,1,0,1],\n",
    "        [0,1,2,2,2],\n",
    "        [2,1,0,0,1]]])\n",
    "    b = np.array(\n",
    "        [[[0,1,1],\n",
    "        [2,2,2],\n",
    "        [1,0,0]],\n",
    "        [[1,0,2],\n",
    "        [0,0,0],\n",
    "        [1,2,1]]])\n",
    "    cl = ConvLayer(5,5,3,3,3,2,1,2,IdentityActivator(),0.001)\n",
    "    cl.filters[0].weights = np.array(\n",
    "        [[[-1,1,0],\n",
    "        [0,1,0],\n",
    "        [0,1,1]],\n",
    "        [[-1,-1,0],\n",
    "        [0,0,0],\n",
    "        [0,-1,0]],\n",
    "        [[0,0,-1],\n",
    "        [0,1,0],\n",
    "        [1,-1,-1]]], dtype=np.float64)\n",
    "    cl.filters[0].bias = 1\n",
    "    cl.filters[1].weights = np.array(\n",
    "        [[[1,1,-1],\n",
    "        [-1,-1,1],\n",
    "        [0,-1,1]],\n",
    "        [[0,1,0],\n",
    "        [-1,0,-1],\n",
    "        [-1,1,0]],\n",
    "        [[-1,0,0],\n",
    "        [-1,0,1],\n",
    "        [-1,0,0]]], dtype=np.float64)\n",
    "    return a, b, cl\n",
    "\n",
    "def gradient_check():\n",
    "    '''\n",
    "    梯度检查\n",
    "    '''\n",
    "    # 设计一个误差函数，取所有节点输出项之和\n",
    "    error_function = lambda o: o.sum()\n",
    "\n",
    "    # 计算forward值\n",
    "    a, b, cl = init_test()\n",
    "    cl.forward(a)\n",
    "\n",
    "    # 求取sensitivity map，是一个全1数组\n",
    "    sensitivity_array = np.ones(cl.output_array.shape, dtype=np.float64)\n",
    "\n",
    "    # 计算梯度\n",
    "    cl.backward(a, sensitivity_array, IdentityActivator())\n",
    "\n",
    "    # 检查梯度\n",
    "    epsilon = 10e-4\n",
    "    for d in range(cl.filters[0].weights_grad.shape[0]):\n",
    "        for i in range(cl.filters[0].weights_grad.shape[1]):\n",
    "            for j in range(cl.filters[0].weights_grad.shape[2]):\n",
    "                cl.filters[0].weights[d, i, j] += epsilon\n",
    "                cl.forward(a)\n",
    "                err1 = error_function(cl.output_array)\n",
    "                cl.filters[0].weights[d, i, j] -= 2 * epsilon\n",
    "                cl.forward(a)\n",
    "                err2 = error_function(cl.output_array)\n",
    "                expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "                cl.filters[0].weights[d, i, j] += epsilon\n",
    "                print('weights(%d,%d,%d): expected - actual %f - %f' % (\n",
    "                    d, i, j, expect_grad, cl.filters[0].weights_grad[d, i, j]))\n",
    "\n",
    "# 调用梯度检查函数\n",
    "gradient_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795449f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbd07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
