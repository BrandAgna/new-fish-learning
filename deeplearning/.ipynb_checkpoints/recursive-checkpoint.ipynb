{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89c27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activators import IdentityActivator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1a21a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    def __init__(self, data, children=[], children_data=[]):\n",
    "        self.parent = None\n",
    "        self.children = children\n",
    "        self.children_data = children_data\n",
    "        self.data = data\n",
    "        for child in children:\n",
    "            child.parent = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b287b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 递归神经网络实现\n",
    "class RecursiveLayer(object):\n",
    "    def __init__(self, node_width, child_count,\n",
    "                 activator, learning_rate):\n",
    "        '''\n",
    "        递归神经网络构造函数\n",
    "        node_width: 表示每个节点的向量的维度\n",
    "        child_count: 每个父节点有几个子节点\n",
    "        activator: 激活函数对象\n",
    "        learning_rate: 梯度下降算法学习率\n",
    "        '''\n",
    "        self.node_width = node_width\n",
    "        self.child_count = child_count\n",
    "        self.activator = activator\n",
    "        self.learning_rate = learning_rate\n",
    "        # 权重数组W\n",
    "        self.W = np.random.uniform(-1e-4, 1e-4,\n",
    "            (node_width, node_width * child_count))\n",
    "        # 偏置项b\n",
    "        self.b = np.zeros((node_width, 1))\n",
    "        # 递归神经网络生成的树的根节点\n",
    "        self.root = None\n",
    "\n",
    "    def forward(self, *children):\n",
    "        '''\n",
    "        前向计算\n",
    "        '''\n",
    "        children_data = self.concatenate(children)\n",
    "        parent_data = self.activator.forward(\n",
    "            np.dot(self.W, children_data) + self.b\n",
    "        )\n",
    "        self.root = TreeNode(parent_data, children                           , children_data)\n",
    "    def concatenate(self, tree_nodes):\n",
    "        '''\n",
    "        将各个树节点中的数据拼接成一个长向量\n",
    "        '''\n",
    "        concat = np.zeros((0,1))\n",
    "        for node in tree_nodes:\n",
    "            concat = np.concatenate((concat, node.data))\n",
    "        return concat\n",
    "\n",
    "    def backward(self, parent_delta):\n",
    "        '''\n",
    "        BPTS反向传播算法\n",
    "        '''\n",
    "        self.calc_delta(parent_delta, self.root)\n",
    "        self.W_grad, self.b_grad = self.calc_gradient(self.root)\n",
    "\n",
    "    def calc_delta(self, parent_delta, parent):\n",
    "        '''\n",
    "        计算每个节点的delta\n",
    "        '''\n",
    "        parent.delta = parent_delta\n",
    "        if parent.children:\n",
    "            # 根据式2计算每个子节点的delta\n",
    "            children_delta = np.dot(self.W.T, parent_delta) * (\n",
    "                self.activator.backward(parent.children_data)\n",
    "            )\n",
    "            # slices = [(子节点编号，子节点delta起始位置，子节点delta结束位置)]\n",
    "            slices = [(i, i * self.node_width,\n",
    "                        (i + 1) * self.node_width)\n",
    "                        for i in range(self.child_count)]\n",
    "            # 针对每个子节点，递归调用calc_delta函数\n",
    "            for s in slices:\n",
    "                self.calc_delta(children_delta[s[1]:s[2]],\n",
    "                                parent.children[s[0]])\n",
    "\n",
    "    def calc_gradient(self, parent):\n",
    "        '''\n",
    "        计算每个节点权重的梯度，并将它们求和，得到最终的梯度\n",
    "        '''\n",
    "        W_grad = np.zeros((self.node_width,\n",
    "                            self.node_width * self.child_count))\n",
    "        b_grad = np.zeros((self.node_width, 1))\n",
    "        if not parent.children:\n",
    "            return W_grad, b_grad\n",
    "        parent.W_grad = np.dot(parent.delta, parent.children_data.T)\n",
    "        parent.b_grad = parent.delta\n",
    "        W_grad += parent.W_grad\n",
    "        b_grad += parent.b_grad\n",
    "        for child in parent.children:\n",
    "            W, b = self.calc_gradient(child)\n",
    "            W_grad += W\n",
    "            b_grad += b\n",
    "        return W_grad, b_grad\n",
    "\n",
    "    def update(self):\n",
    "        '''\n",
    "        使用SGD算法更新权重\n",
    "        '''\n",
    "        self.W -= self.learning_rate * self.W_grad\n",
    "        self.b -= self.learning_rate * self.b_grad\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.root = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79150c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set():\n",
    "    children = [\n",
    "        TreeNode(np.array([[1],[2]])),\n",
    "        TreeNode(np.array([[3],[4]])),\n",
    "        TreeNode(np.array([[5],[6]]))\n",
    "    ]\n",
    "    d = np.array([[0.5],[0.8]])\n",
    "    return children, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2df61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check():\n",
    "    '''\n",
    "    梯度检查\n",
    "    '''\n",
    "    # 设计一个误差函数，取所有节点输出项之和\n",
    "    error_function = lambda o: o.sum()\n",
    "\n",
    "    rnn = RecursiveLayer(2, 2, IdentityActivator(), 1e-3)\n",
    "\n",
    "    # 计算forward值\n",
    "    x, d = data_set()\n",
    "    rnn.forward(x[0], x[1])\n",
    "    rnn.forward(rnn.root, x[2])\n",
    "\n",
    "    # 求取sensitivity map\n",
    "    sensitivity_array = np.ones((rnn.node_width, 1),\n",
    "                                dtype=np.float64)\n",
    "    # 计算梯度\n",
    "    rnn.backward(sensitivity_array)\n",
    "\n",
    "    # 检查梯度\n",
    "    epsilon = 10e-4\n",
    "    for i in range(rnn.W.shape[0]):\n",
    "        for j in range(rnn.W.shape[1]):\n",
    "            rnn.W[i, j] += epsilon\n",
    "            rnn.reset_state()\n",
    "            rnn.forward(x[0], x[1])\n",
    "            rnn.forward(rnn.root, x[2])\n",
    "            err1 = error_function(rnn.root.data)\n",
    "            rnn.W[i, j] -= 2 * epsilon\n",
    "            rnn.reset_state()\n",
    "            rnn.forward(x[0], x[1])\n",
    "            rnn.forward(rnn.root, x[2])\n",
    "            err2 = error_function(rnn.root.data)\n",
    "            expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "            rnn.W[i, j] += epsilon\n",
    "            print('weights(%d,%d): expected - actural %.4e - %.4e' % (i, j, expect_grad, rnn.W_grad[i, j]))\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9fa487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights(0,0): expected - actural 1.6719e-04 - 1.6719e-04\n",
      "weights(0,1): expected - actural -6.5030e-05 - -6.5030e-05\n",
      "weights(0,2): expected - actural 4.9999e+00 - 4.9999e+00\n",
      "weights(0,3): expected - actural 5.9999e+00 - 5.9999e+00\n",
      "weights(1,0): expected - actural 9.9404e-05 - 9.9404e-05\n",
      "weights(1,1): expected - actural -2.0061e-04 - -2.0061e-04\n",
      "weights(1,2): expected - actural 4.9997e+00 - 4.9997e+00\n",
      "weights(1,3): expected - actural 5.9996e+00 - 5.9996e+00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gradient_check()  # 确保这个函数是正确定义并且可以被调用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67fe10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
