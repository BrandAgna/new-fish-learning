{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5166f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cnn import ReluActivator, IdentityActivator, element_wise_op\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37709976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLayer(object):\n",
    "    def __init__(self, input_width, state_width, activator, learning_rate):\n",
    "        self.input_width = input_width\n",
    "        self.state_width = state_width\n",
    "        self.activator = activator\n",
    "        self.learning_rate = learning_rate\n",
    "        self.times = 0  # 当前时刻初始化为t0\n",
    "        self.state_list = []  # 保存各个时刻的state\n",
    "        self.state_list.append(np.zeros((state_width, 1)))  # 初始化s0\n",
    "        self.U = np.random.uniform(-1e-4, 1e-4, (state_width, input_width))  # 初始化U\n",
    "        self.W = np.random.uniform(-1e-4, 1e-4, (state_width, state_width))  # 初始化W\n",
    "\n",
    "# 损失函数计算说明\n",
    "# L(y, o) = − log 1/N ∑n∈N yn on\n",
    "# 例如 y1 = [1, 0, 0, 0], o = [0.03, 0.09, 0.24, 0.64]\n",
    "# L = − 1/N log ∑n∈N yn on = −y1 logo1\n",
    "# = −(1 * log0.03 + 0 * log0.09 + 0 * log0.24 + 0 * log0.64)\n",
    "# = 3.51\n",
    "\n",
    "    def forward(self, input_array):\n",
    "        '''\n",
    "        根据『式2』进行前向计算\n",
    "        '''\n",
    "        self.times += 1\n",
    "        state = (np.dot(self.U, input_array) + np.dot(self.W, self.state_list[-1]))\n",
    "        element_wise_op(state, self.activator.forward)\n",
    "        self.state_list.append(state)\n",
    "\n",
    "    def backward(self, sensitivity_array, activator):\n",
    "        '''\n",
    "        实现BPTT算法\n",
    "        '''\n",
    "        self.calc_delta(sensitivity_array, activator)\n",
    "        self.calc_gradient()\n",
    "\n",
    "    def calc_delta(self, sensitivity_array, activator):\n",
    "        self.delta_list = []  # 用来保存各个时刻的误差项\n",
    "        for i in range(self.times):\n",
    "            self.delta_list.append(np.zeros((self.state_width, 1)))\n",
    "        self.delta_list.append(sensitivity_array)\n",
    "        # 迭代计算每个时刻的误差项\n",
    "        for k in range(self.times - 1, 0, -1):\n",
    "            self.calc_delta_k(k, activator)\n",
    "\n",
    "    def calc_delta_k(self, k, activator):\n",
    "        '''\n",
    "        根据k+1时刻的delta计算k时刻的delta\n",
    "        '''\n",
    "        state = self.state_list[k + 1].copy()\n",
    "        element_wise_op(self.state_list[k + 1], activator.backward)\n",
    "        self.delta_list[k] = np.dot(np.dot(self.delta_list[k + 1].T, self.W), np.diag(state[:, 0])).T\n",
    "\n",
    "    def calc_gradient(self):\n",
    "        self.gradient_list = []  # 保存各个时刻的权重梯度\n",
    "        for t in range(self.times + 1):\n",
    "            self.gradient_list.append(np.zeros((self.state_width, self.state_width)))\n",
    "        for t in range(self.times, 0, -1):\n",
    "            self.calc_gradient_t(t)\n",
    "        # 实际的梯度是各个时刻梯度之和\n",
    "        self.gradient = reduce(lambda a, b: a + b, self.gradient_list, self.gradient_list[0])  # [0]被初始化为0且没有被修改过\n",
    "\n",
    "    def calc_gradient_t(self, t):\n",
    "        '''\n",
    "        计算每个时刻t权重的梯度\n",
    "        '''\n",
    "        gradient = np.dot(self.delta_list[t], self.state_list[t - 1].T)\n",
    "        self.gradient_list[t] = gradient\n",
    "\n",
    "    def update(self):\n",
    "        '''\n",
    "        按照梯度下降，更新权重\n",
    "        '''\n",
    "        self.W -= self.learning_rate * self.gradient\n",
    "\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.times = 0  # 当前时刻初始化为t0\n",
    "        self.state_list = []  # 保存各个时刻的state\n",
    "        self.state_list.append(np.zeros(\n",
    "            (self.state_width, 1)))  # 初始化s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78aa06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set():\n",
    "    x = [np.array([[1], [2], [3]]),\n",
    "         np.array([[2], [3], [4]])]\n",
    "    d = np.array([[1], [2]])\n",
    "    return x, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e5777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check():\n",
    "    '''\n",
    "    梯度检查\n",
    "    '''\n",
    "    # 设计一个误差函数，取所有节点输出项之和\n",
    "    error_function = lambda o: o.sum()\n",
    "\n",
    "    rl = RecurrentLayer(3, 2, IdentityActivator(), 1e-3)\n",
    "\n",
    "    # 计算forward值\n",
    "    x, d = data_set()\n",
    "    rl.forward(x[0])\n",
    "    rl.forward(x[1])\n",
    "\n",
    "    # 求取sensitivity map\n",
    "    sensitivity_array = np.ones(rl.state_list[-1].shape,\n",
    "                                dtype=np.float64)\n",
    "    # 计算梯度\n",
    "    rl.backward(sensitivity_array, IdentityActivator())\n",
    "\n",
    "    # 检查梯度\n",
    "    epsilon = 10e-4\n",
    "    for i in range(rl.W.shape[0]):\n",
    "        for j in range(rl.W.shape[1]):\n",
    "            rl.W[i, j] += epsilon\n",
    "            rl.reset_state()\n",
    "            rl.forward(x[0])\n",
    "            rl.forward(x[1])\n",
    "            err1 = error_function(rl.state_list[-1])\n",
    "            rl.W[i, j] -= 2 * epsilon\n",
    "            rl.reset_state()\n",
    "            rl.forward(x[0])\n",
    "            rl.forward(x[1])\n",
    "            err2 = error_function(rl.state_list[-1])\n",
    "            expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "            rl.W[i, j] += epsilon\n",
    "            print(f'weights({i},{j}): expected - actual {expect_grad:.6f} - {rl.gradient[i, j]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ebcb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    l = RecurrentLayer(3, 2, ReluActivator(), 1e-3)\n",
    "    x, d = data_set()\n",
    "    l.forward(x[0])\n",
    "    l.forward(x[1])\n",
    "    l.backward(d, ReluActivator())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd7994c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights(0,0): expected - actual -0.000418 - -0.000418\n",
      "weights(0,1): expected - actual -0.000135 - -0.000135\n",
      "weights(1,0): expected - actual -0.000418 - -0.000418\n",
      "weights(1,1): expected - actual -0.000135 - -0.000135\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gradient_check()  # 确保这个函数是正确定义并且可以被调用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389f9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
